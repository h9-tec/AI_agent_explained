# Course Extension Summary: llama.cpp + CrewAI Integration

**By Hesham Haroon**

## Overview

This document summarizes the extensions made to the AI Agents course to support **llama.cpp** (local models) and the **CrewAI** framework for multi-agent orchestration.

---

## What's New

### 1. **Unified LLM Backend**
- Support for both OpenAI API and llama.cpp
- Seamless switching via configuration
- Consistent interface across all modules

### 2. **llama.cpp Integration**
- Run models locally without API costs
- Privacy-focused (no data sent to external servers)
- Support for GPU acceleration (CUDA, Metal)
- Works with GGUF quantized models

### 3. **CrewAI Framework**
- Multi-agent orchestration
- Role-based agent collaboration
- Sequential and hierarchical workflows
- Pre-built tools and integrations

### 4. **Enhanced Configuration**
- Environment-based configuration (.env)
- Easy model switching
- GPU/CPU configuration
- Comprehensive validation

---

## New Files

### Core Infrastructure

| File | Purpose |
|------|---------|
| `config.py` | Configuration management and validation |
| `llm_client.py` | Unified LLM client (OpenAI + llama.cpp) |
| `requirements.txt` | All dependencies including llama-cpp-python and CrewAI |
| `.env.example` | Example environment configuration |

### Extended Modules (llama.cpp versions)

| File | Description |
|------|-------------|
| `module_0_agent_llamacpp.py` | Basic agent loop with llama.cpp |
| `module_1_tool_agent_llamacpp.py` | Tool use with function calling |
| `module_2_memory_agent_llamacpp.py` | Memory and state management |
| `module_3_react_agent_llamacpp.py` | ReAct pattern implementation |

### New Module

| File | Description |
|------|-------------|
| `module_4_crewai_agents.py` | Multi-agent orchestration with CrewAI |
| `module_4_crewai.md` | Documentation for Module 4 |

### Documentation

| File | Purpose |
|------|---------|
| `SETUP_GUIDE.md` | Complete setup instructions |
| `EXTENSION_SUMMARY.md` | This file |

---

## Architecture

### Unified Client Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Your Agent Code                 â”‚
â”‚  (module_0, module_1, module_2, etc.)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         UnifiedClient                   â”‚
â”‚    (llm_client.py)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  OpenAI-compatible interface    â”‚   â”‚
â”‚  â”‚  client.chat.completions.create â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAIClient â”‚    â”‚LlamaCppClientâ”‚
â”‚              â”‚    â”‚              â”‚
â”‚ â€¢ API calls  â”‚    â”‚ â€¢ Local inf. â”‚
â”‚ â€¢ Function   â”‚    â”‚ â€¢ GGUF modelsâ”‚
â”‚   calling    â”‚    â”‚ â€¢ GPU supportâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration Flow

```
.env file
   â”‚
   â–¼
config.py (loads and validates)
   â”‚
   â–¼
llm_client.py (selects backend)
   â”‚
   â–¼
Your modules (use unified interface)
```

---

## Key Features

### 1. Backend Switching

Switch between OpenAI and llama.cpp by changing one line in `.env`:

```bash
# Use local models
LLM_BACKEND=llamacpp

# Or use OpenAI
LLM_BACKEND=openai
```

### 2. Function Calling Support

Both backends support function calling/tool use:

**OpenAI:** Native function calling API  
**llama.cpp:** Custom implementation with prompt engineering

### 3. GPU Acceleration

llama.cpp supports multiple acceleration backends:

- **NVIDIA GPUs:** CUDA
- **Apple Silicon:** Metal
- **AMD GPUs:** ROCm (experimental)
- **CPU:** Optimized inference

### 4. Model Flexibility

Use any GGUF model from Hugging Face:

- Llama 2 (7B, 13B, 70B)
- Mistral (7B, 8x7B)
- CodeLlama (7B, 13B, 34B)
- Phi-2, Gemma, and more

---

## Comparison: OpenAI vs. llama.cpp

| Feature | OpenAI | llama.cpp |
|---------|--------|-----------|
| **Cost** | Pay per token | Free (after model download) |
| **Privacy** | Data sent to OpenAI | Fully local |
| **Speed** | Fast (cloud) | Depends on hardware |
| **Quality** | Excellent | Good (model-dependent) |
| **Setup** | Easy (API key) | Moderate (model download) |
| **Internet** | Required | Not required |
| **Customization** | Limited | Full control |

---

## Module Comparison

### Original Modules (OpenAI only)

- `module_0_agent.py`
- `module_1_tool_agent.py`
- `module_2_memory_agent.py`
- `module_3_react_agent.py`

**Pros:** Simple, proven, well-tested  
**Cons:** Requires OpenAI API key, costs money

### Extended Modules (Unified backend)

- `module_0_agent_llamacpp.py`
- `module_1_tool_agent_llamacpp.py`
- `module_2_memory_agent_llamacpp.py`
- `module_3_react_agent_llamacpp.py`

**Pros:** Works with both backends, flexible, cost-effective  
**Cons:** Slightly more complex setup

---

## CrewAI Integration

### What is CrewAI?

A framework for building multi-agent systems where agents collaborate like a crew.

### Key Concepts

1. **Agents:** Specialized AI entities with roles and goals
2. **Tasks:** Work to be done by agents
3. **Crew:** Orchestration of agents and tasks
4. **Process:** Execution strategy (sequential, hierarchical)

### Example Use Cases

- **Research teams:** Researcher + Analyst + Writer
- **Software teams:** Architect + Developer + QA
- **Content teams:** SEO Specialist + Writer + Editor
- **Support teams:** Triage + Specialist + Reviewer

### Why CrewAI?

- **Specialization:** Each agent focuses on what they do best
- **Quality:** Multiple perspectives improve outcomes
- **Scalability:** Easy to add more agents
- **Coordination:** Built-in orchestration
- **Reusability:** Define agents once, use in multiple crews

---

## Getting Started

### Quick Start (5 minutes)

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Download a model
mkdir models
wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf -O models/llama-2-7b-chat.Q4_K_M.gguf

# 3. Configure
cp .env.example .env
# Edit .env to set LLAMA_MODEL_PATH

# 4. Test
python config.py
python module_0_agent_llamacpp.py
```

### Full Setup

See `SETUP_GUIDE.md` for comprehensive instructions.

---

## Performance Tips

### For CPU Inference
- Use Q4 quantized models (4GB)
- Set `LLAMA_N_THREADS` to CPU core count
- Keep `LLAMA_N_CTX` at 2048-4096
- Use smaller models (7B)

### For GPU Inference
- Set `LLAMA_N_GPU_LAYERS` based on VRAM
- Use Q4 or Q5 quantization
- Increase `LLAMA_N_CTX` for longer contexts
- Can use larger models (13B, 70B)

### For Apple Silicon
- Install with Metal support
- Set `LLAMA_N_GPU_LAYERS=1`
- 8GB RAM: 7B models
- 16GB+ RAM: 13B models

---

## Cost Analysis

### OpenAI (GPT-4 Turbo)
- Input: $10 per 1M tokens
- Output: $30 per 1M tokens
- **Example:** 100 agent runs Ã— 10K tokens = $4-12

### llama.cpp (Local)
- Model download: One-time (free)
- Electricity: ~$0.01-0.10 per hour
- Hardware: Use existing computer
- **Example:** 100 agent runs = $0.01-0.10

**Savings:** 99%+ for high-volume usage

---

## Use Cases

### Best for OpenAI
- Prototyping and testing
- Highest quality requirements
- Low volume usage
- No local hardware
- Need latest models

### Best for llama.cpp
- High volume usage
- Privacy requirements
- Cost optimization
- Offline operation
- Full control needed

### Best for Both
- Development: llama.cpp
- Production: OpenAI
- Or vice versa!

---

## Limitations and Considerations

### llama.cpp Limitations
- Slower than cloud APIs (on CPU)
- Quality depends on model choice
- Requires model downloads (10GB+)
- Function calling less robust than OpenAI
- Needs more prompt engineering

### CrewAI Considerations
- More LLM calls = higher cost/time
- Complexity increases with agent count
- Debugging can be challenging
- Not needed for simple tasks
- Works best with capable models

---

## Roadmap

### Completed âœ…
- [x] Unified LLM client
- [x] llama.cpp integration
- [x] Configuration system
- [x] Extended modules 0-3
- [x] CrewAI integration (Module 4)
- [x] Comprehensive documentation

### Future Enhancements ğŸš€
- [ ] AutoGen integration (alternative to CrewAI)
- [ ] LangGraph integration
- [ ] Vector database support (ChromaDB, FAISS)
- [ ] Streaming responses
- [ ] Batch processing
- [ ] Web UI for agent interaction
- [ ] More example projects
- [ ] Performance benchmarks

---

## Contributing

Want to extend this course further? Ideas:

1. **Add more frameworks:** AutoGen, LangGraph, etc.
2. **Add more models:** Support for other model formats
3. **Add more tools:** Web scraping, database access, etc.
4. **Add more examples:** Real-world use cases
5. **Improve documentation:** Tutorials, videos, etc.

---

## Troubleshooting

### Common Issues

1. **Model not found:** Check path in `.env`
2. **Out of memory:** Use smaller model or reduce context
3. **Slow inference:** Increase threads or use GPU
4. **Import errors:** Reinstall dependencies
5. **CrewAI issues:** Try OpenAI backend first

See `SETUP_GUIDE.md` for detailed troubleshooting.

---

## Resources

### Documentation
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- [CrewAI](https://docs.crewai.com/)
- [Hugging Face GGUF Models](https://huggingface.co/models?library=gguf)

### Community
- [llama.cpp Discord](https://discord.gg/llamacpp)
- [CrewAI Discord](https://discord.gg/crewai)
- [r/LocalLLaMA](https://reddit.com/r/LocalLLaMA)

---

## Conclusion

This extension transforms the AI Agents course from an OpenAI-only tutorial into a flexible, production-ready framework that supports:

âœ… **Local and cloud models**  
âœ… **Cost-effective operation**  
âœ… **Privacy-focused deployment**  
âœ… **Multi-agent orchestration**  
âœ… **Real-world scalability**

You now have the tools to build agents that can run anywhere, from your laptop to production servers, using the best model for your needs.

**Happy building!** ğŸš€

